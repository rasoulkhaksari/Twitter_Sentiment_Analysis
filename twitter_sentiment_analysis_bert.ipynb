{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_twitter_bert_fgpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyDODoaWC6KI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a9a3a9-08d3-46e0-a915-81b1bf007c99"
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYFSFV1iopQ0"
      },
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import preprocessor as twpreprocessor\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import random\n",
        "import numpy as np "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSVZ1YCaor10",
        "outputId": "533528dc-75ad-4ff7-c908-58d2fd3537a7"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "w-ZoHFUGou4U",
        "outputId": "a65ff448-c2cb-43b0-ecba-8034e226b495"
      },
      "source": [
        "train = pd.read_csv('https://github.com/hafidhfikri/Practice-Twitter-Sentiment-Analysis/blob/master/train_E6oV3lV.csv?raw=true')\n",
        "print(train.info())\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 31962 entries, 0 to 31961\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      31962 non-null  int64 \n",
            " 1   label   31962 non-null  int64 \n",
            " 2   tweet   31962 non-null  object\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 749.2+ KB\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>@user when a father is dysfunctional and is s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>bihday your majesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>#model   i love u take with u all the time in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>factsguide: society now    #motivation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label                                              tweet\n",
              "0   1      0   @user when a father is dysfunctional and is s...\n",
              "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
              "2   3      0                                bihday your majesty\n",
              "3   4      0  #model   i love u take with u all the time in ...\n",
              "4   5      0             factsguide: society now    #motivation"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWyd3P1Goxq_"
      },
      "source": [
        "limit = 20000\n",
        "positives=train[train.label==0][['label','tweet']].head(limit)\n",
        "negatives=train[train.label==1][['label','tweet']].head(limit)\n",
        "combination=pd.concat([positives,negatives],axis=0,ignore_index=True)\n",
        "\n",
        "twpreprocessor.set_options(twpreprocessor.OPT.EMOJI,twpreprocessor.OPT.ESCAPE_CHAR,twpreprocessor.OPT.MENTION,twpreprocessor.OPT.NUMBER,twpreprocessor.OPT.RESERVED,twpreprocessor.OPT.SMILEY,twpreprocessor.OPT.URL)\n",
        "combination['tidy_tweet'] = np.vectorize(twpreprocessor.clean)(combination['tweet']) \n",
        "combination.tidy_tweet = combination.tidy_tweet.apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "\n",
        "tweets = combination.tweet.values\n",
        "labels = combination.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYW1ul-Ao03V",
        "outputId": "9f88cd1c-8466-4482-d0e5-89f2337fb4ba"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tweetid = []\n",
        "for tweet in tweets:\n",
        "    encoded_tweet = tokenizer.encode(tweet,add_special_tokens = True)\n",
        "    tweetid.append(encoded_tweet)\n",
        "\n",
        "print('Original: ', tweets[0])\n",
        "print('Token IDs:', tweetid[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:   @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
            "Token IDs: [101, 1030, 5310, 2043, 1037, 2269, 2003, 28466, 2389, 1998, 2003, 2061, 14337, 2002, 8011, 2015, 2010, 4268, 2046, 2010, 28466, 1012, 1001, 2448, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS324MkKo338",
        "outputId": "6da6ff94-c593-4e00-ca14-1db3369c6a21"
      },
      "source": [
        "MAX_LEN = 64\n",
        "print(f'\\n Truncating all sentences to {MAX_LEN} values...')\n",
        "print(f'\\nPadding token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}')\n",
        "tweetid = pad_sequences(tweetid, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: [PAD], ID: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLbR5Xtno6Hz"
      },
      "source": [
        "masks = []\n",
        "for tweet in tweetid:\n",
        "    mask = [int(token_id > 0) for token_id in tweet]\n",
        "    masks.append(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnNi8nKyo7vr"
      },
      "source": [
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(tweetid, labels, random_state=2018, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(masks, labels, random_state=2018, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2EreIe0o-gl"
      },
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jDVGmh-pB69",
        "outputId": "2369d02a-54cb-47b6-8767-6aaeaedae067"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels = 2,output_attentions = False,output_hidden_states = False)\n",
        "optimizer = AdamW(model.parameters(),lr = 2e-5,eps = 1e-8)\n",
        "epochs = 4\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps = 0,num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qF8TUeOwere",
        "outputId": "a755e14c-83f6-41ee-b95c-a47cb26a8248"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udnGyVjQpGlh",
        "outputId": "9d7630c3-931b-4141-c216-63a1e5015b08"
      },
      "source": [
        "def accuracy(preds, labels):\n",
        "  pred = np.argmax(preds, axis=1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  return np.sum(pred == labels) / len(labels)\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "loss_values = []\n",
        "for epoch_i in range(0, epochs):\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "  total_loss = 0\n",
        "  model.train()\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "\n",
        "    model.zero_grad()        \n",
        "\n",
        "    outputs = model(b_input_ids, \n",
        "                token_type_ids=None, \n",
        "                attention_mask=b_input_mask, \n",
        "                labels=b_labels)\n",
        "\n",
        "    loss = outputs[0]    \n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"validation\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "for batch in validation_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():        \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "    logits = outputs[0]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Epoch 1 / 4 ========\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.05\n",
            "validation\n",
            "  Average training loss: 0.05\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.09\n",
            "validation\n",
            "  Average training loss: 0.09\n",
            "validation\n",
            "  Average training loss: 0.09\n",
            "validation\n",
            "  Average training loss: 0.10\n",
            "validation\n",
            "  Average training loss: 0.10\n",
            "validation\n",
            "  Average training loss: 0.10\n",
            "validation\n",
            "  Average training loss: 0.11\n",
            "validation\n",
            "  Average training loss: 0.11\n",
            "validation\n",
            "  Average training loss: 0.11\n",
            "validation\n",
            "  Average training loss: 0.11\n",
            "validation\n",
            "  Average training loss: 0.12\n",
            "validation\n",
            "  Average training loss: 0.12\n",
            "validation\n",
            "  Average training loss: 0.12\n",
            "validation\n",
            "  Average training loss: 0.12\n",
            "validation\n",
            "  Average training loss: 0.13\n",
            "validation\n",
            "  Average training loss: 0.13\n",
            "validation\n",
            "  Average training loss: 0.13\n",
            "validation\n",
            "  Average training loss: 0.13\n",
            "validation\n",
            "  Average training loss: 0.14\n",
            "validation\n",
            "  Average training loss: 0.14\n",
            "validation\n",
            "  Average training loss: 0.14\n",
            "validation\n",
            "  Average training loss: 0.14\n",
            "validation\n",
            "  Average training loss: 0.15\n",
            "validation\n",
            "  Average training loss: 0.15\n",
            "validation\n",
            "  Average training loss: 0.15\n",
            "validation\n",
            "  Average training loss: 0.15\n",
            "validation\n",
            "  Average training loss: 0.15\n",
            "validation\n",
            "  Average training loss: 0.16\n",
            "validation\n",
            "  Average training loss: 0.16\n",
            "validation\n",
            "  Average training loss: 0.16\n",
            "validation\n",
            "  Average training loss: 0.16\n",
            "validation\n",
            "  Average training loss: 0.16\n",
            "validation\n",
            "  Average training loss: 0.17\n",
            "validation\n",
            "  Average training loss: 0.17\n",
            "validation\n",
            "  Batch    50  of     79. \n",
            "  Average training loss: 0.17\n",
            "validation\n",
            "  Average training loss: 0.17\n",
            "validation\n",
            "  Average training loss: 0.17\n",
            "validation\n",
            "  Average training loss: 0.17\n",
            "validation\n",
            "  Average training loss: 0.18\n",
            "validation\n",
            "  Average training loss: 0.18\n",
            "validation\n",
            "  Average training loss: 0.18\n",
            "validation\n",
            "  Average training loss: 0.18\n",
            "validation\n",
            "  Average training loss: 0.18\n",
            "validation\n",
            "  Average training loss: 0.19\n",
            "validation\n",
            "  Average training loss: 0.19\n",
            "validation\n",
            "  Average training loss: 0.19\n",
            "validation\n",
            "  Average training loss: 0.19\n",
            "validation\n",
            "  Average training loss: 0.19\n",
            "validation\n",
            "  Average training loss: 0.20\n",
            "validation\n",
            "  Average training loss: 0.20\n",
            "validation\n",
            "  Average training loss: 0.20\n",
            "validation\n",
            "  Average training loss: 0.20\n",
            "validation\n",
            "  Average training loss: 0.20\n",
            "validation\n",
            "  Average training loss: 0.21\n",
            "validation\n",
            "  Average training loss: 0.21\n",
            "validation\n",
            "  Average training loss: 0.21\n",
            "validation\n",
            "  Average training loss: 0.21\n",
            "validation\n",
            "  Average training loss: 0.21\n",
            "validation\n",
            "  Average training loss: 0.21\n",
            "validation\n",
            "  Average training loss: 0.22\n",
            "validation\n",
            "  Average training loss: 0.22\n",
            "validation\n",
            "  Average training loss: 0.22\n",
            "validation\n",
            "  Average training loss: 0.22\n",
            "validation\n",
            "======== Epoch 2 / 4 ========\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.05\n",
            "validation\n",
            "  Average training loss: 0.05\n",
            "validation\n",
            "  Average training loss: 0.05\n",
            "validation\n",
            "  Average training loss: 0.05\n",
            "validation\n",
            "  Average training loss: 0.05\n",
            "validation\n",
            "  Average training loss: 0.05\n",
            "validation\n",
            "  Average training loss: 0.05\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Batch    50  of     79. \n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.06\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.07\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.08\n",
            "validation\n",
            "  Average training loss: 0.09\n",
            "validation\n",
            "  Average training loss: 0.09\n",
            "validation\n",
            "  Average training loss: 0.09\n",
            "validation\n",
            "  Average training loss: 0.09\n",
            "validation\n",
            "  Average training loss: 0.09\n",
            "validation\n",
            "  Average training loss: 0.09\n",
            "validation\n",
            "  Average training loss: 0.09\n",
            "validation\n",
            "======== Epoch 3 / 4 ========\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Batch    50  of     79. \n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.03\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "  Average training loss: 0.04\n",
            "validation\n",
            "======== Epoch 4 / 4 ========\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.00\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Batch    50  of     79. \n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.01\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Average training loss: 0.02\n",
            "validation\n",
            "  Accuracy: 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}